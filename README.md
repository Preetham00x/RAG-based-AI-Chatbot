ğŸ“š RAG-Based AI Chatbot (Next.js + ChromaDB + Groq LLM)

A fully functional Retrieval-Augmented Generation (RAG) chatbot built using:

Next.js 14 (App Router)

Groq LLaMA-3 API for LLM responses

ChromaDB for vector storage

Custom document upload + embedding pipeline

This chatbot allows you to upload documents, automatically embed them into vectors, store them in ChromaDB, and then ask questions that the bot answers using retrieved context.

ğŸš€ Features
âœ… Upload Documents

Upload .txt, .md, .pdf (optional), code files, and more.

âœ… Automatic Vector Embeddings

Uploaded docs are embedded using a Groq-based embedding prompt.

âœ… ChromaDB Vector Storage

Stores all embeddings + documents in a persistent local vector DB.

âœ… Intelligent Querying

User questions â†’ Chroma retrieves similar chunks â†’ Sent to LLaMA-3 for a grounded answer.

âœ… Chat Interface

Simple React component for chat exchange.

âœ… API Routes

/api/embed â†’ Upload & embed documents

/api/query â†’ Query using RAG

/api/chat â†’ Normal LLM chat

ğŸ§© Project Structure
next-rag-chatbot/
â”œâ”€ app/
â”‚  â”œâ”€ layout.js
â”‚  â”œâ”€ page.js
â”‚  â””â”€ api/
â”‚     â”œâ”€ embed/route.js
â”‚     â”œâ”€ query/route.js
â”‚     â””â”€ chat/route.js
â”œâ”€ components/
â”‚  â”œâ”€ ChatBox.jsx
â”‚  â””â”€ UploadPanel.jsx
â”œâ”€ lib/
â”‚  â”œâ”€ chromaClient.js
â”‚  â””â”€ groqClient.js
â”œâ”€ public/
â”œâ”€ package.json
â”œâ”€ README.md
â””â”€ .env.local

ğŸ› ï¸ Installation & Setup
1ï¸âƒ£ Clone the Repository
git clone https://github.com/yourusername/yourrepo.git
cd next-rag-chatbot

2ï¸âƒ£ Install Dependencies
npm install

3ï¸âƒ£ Setup Environment Variables

Create a file:

.env.local


Add:



4ï¸âƒ£ Start ChromaDB

If using the built-in server:

pip install chromadb
chroma run --path ./chroma


Or using Docker:

docker run -p 8000:8000 chromadb/chroma:latest

5ï¸âƒ£ Run the Next.js App
npm run dev


Runs on:

http://localhost:3000

ğŸ’¡ How It Works
ğŸŸ¦ 1. Document Upload â†’ Embedding

/api/embed route:

receives uploaded file

extracts text

generates an embedding via Groq LLM

stores vector + original text in ChromaDB

ğŸŸ© 2. Querying

/api/query route:

user asks a question

Chroma finds nearest documents

Combined into a context block

Passed to Groq LLaMA-3 for grounded response

ğŸŸ§ 3. Chat (No RAG)

/api/chat route:

pure LLM chat

no retrieval

ğŸ“¸ Screenshots (optional)

(Add after you take screenshots)

ğŸ“Œ Example Query Flow

Input Question:

"Explain the architecture described in the uploaded document."

System Flow:

Search Chroma for similar chunks

Retrieve top 3 relevant pieces

Build a prompt:

Use ONLY the following context to answer:
<context...>

Question: ...


Groq produces the final answer

ğŸ§ª Testing API Routes
Test upload route
curl -X POST -F "file=@notes.txt" http://localhost:3000/api/embed

Test query route
curl -X POST http://localhost:3000/api/query \
  -H "Content-Type: application/json" \
  -d '{"question": "What did the document talk about?"}'

ğŸ“‚ To Add PDF Support

Ask:

â€œAdd PDF text extractionâ€

And I will generate the updated code.

ğŸ§­ Roadmap

 Better chat UI

 Source-citation in answers

 Streaming responses

 PDF + DOCX text parsing

 Authentication

ğŸ¤ Contributing

Pull requests are welcome!
Open an issue if you find bugs or want improvements.

ğŸ“œ License

MIT License â€” free to use, share, and modify.
